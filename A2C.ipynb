{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2C.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bt1h7On7kd3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import os\n",
        "import torch\n",
        "import wandb\n",
        "import torch.optim as optim\n",
        "from itertools import count\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQeEfE3nPn8v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2b0d53c9-6fc0-4a91-cc65-3234e4bd845e"
      },
      "source": [
        "!wandb login 9c6dd4b0a9335b3cfcfbb62569cf65cd4e537266"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHH8i3x-kyjB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "  def __init__(self, d_in):\n",
        "    super(Critic, self).__init__()\n",
        "    self.lin1 = nn.Linear(d_in, 32)\n",
        "    self.lin2 = nn.Linear(32, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.double()\n",
        "    x = F.relu(self.lin1(x))\n",
        "    return self.lin2(x)\n",
        "\n",
        "class Actor(nn.Module):\n",
        "  def __init__(self, d_in, d_out):\n",
        "    super(Actor, self).__init__()\n",
        "    self.lin1 = nn.Linear(d_in, 32)\n",
        "    self.lin2 = nn.Linear(32, d_out)\n",
        "    self.soft = nn.Softmax(dim=0)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.double()\n",
        "    x = F.relu(self.lin1(x))\n",
        "    return self.soft(self.lin2(x))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpL4BvwAmL30",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "b140a52b-15f3-43e7-9faa-524434c780a1"
      },
      "source": [
        "### HYPERPARAMETERS ###\n",
        "\n",
        "wandb.init(project=\"a2c\") # weights & biases tracking\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"using \" + str(device))\n",
        "\n",
        "n_actions = env.action_space.n\n",
        "n_state = env.observation_space.shape[0]-2\n",
        "\n",
        "# actions:{0,1} apply force {+1 move right, -1 move left}\n",
        "# Actor represents policy, given a state provide the probability of taking each action\n",
        "actor = Actor(d_in=n_state, d_out=n_actions).double().to(device)\n",
        "\n",
        "# Critic represents value function, given a state return the estimated value\n",
        "critic = Critic(d_in=n_state).double().to(device)\n",
        "\n",
        "# reward discount factor\n",
        "gamma = 0.99\n",
        "n_episodes = 500\n",
        "lr_actor = .001\n",
        "lr_critic = .005\n",
        "\n",
        "optimizer_actor = optim.Adam(actor.parameters(),lr=lr_actor)\n",
        "optimizer_critic = optim.Adam(critic.parameters(),lr=lr_critic)\n",
        "criterion_actor = nn.CrossEntropyLoss()\n",
        "criterion_critic = nn.MSELoss()\n",
        "\n",
        "wandb.watch(actor)\n",
        "wandb.watch(critic)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/nj-nj23essdf-sl-/a2c\" target=\"_blank\">https://app.wandb.ai/nj-nj23essdf-sl-/a2c</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/nj-nj23essdf-sl-/a2c/runs/2ju8wwoj\" target=\"_blank\">https://app.wandb.ai/nj-nj23essdf-sl-/a2c/runs/2ju8wwoj</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "using cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<wandb.wandb_torch.TorchGraph at 0x7ff1a5ae9908>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IheXCWxqJvds",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7bc05058-2017-419b-cacc-2a7a5b08dd93"
      },
      "source": [
        "for index, episode in enumerate(range(n_episodes)):\n",
        "  state = env.reset()\n",
        "  # only last two state observations are needed: angle and angular velocity\n",
        "  state = torch.DoubleTensor(state)[-2:].to(device)\n",
        "\n",
        "  if (index+1) % 200 == 0:\n",
        "    lr_actor *= .1\n",
        "    optimizer_actor = optim.Adam(actor.parameters(),lr=lr_actor)\n",
        "\n",
        "  I = 1. # anneal policy update\n",
        "\n",
        "  # log progress\n",
        "  acc_loss_actor = 0.0\n",
        "  acc_loss_critic = 0.0\n",
        "\n",
        "  action = actor(state)\n",
        "\n",
        "  for t in count():    \n",
        "    # choose action stochastically\n",
        "    step_action = np.random.choice(n_actions,p=action.detach().cpu().numpy())\n",
        "    state_prime, reward, done, info = env.step(step_action)\n",
        "    done_mask = 0. if done else 1.\n",
        "\n",
        "    done_mask = torch.as_tensor(done_mask).requires_grad_().to(device)\n",
        "    reward = torch.as_tensor(reward).requires_grad_().to(device)\n",
        "    state_prime = torch.tensor(state_prime[-2:], requires_grad=True).to(device)\n",
        "\n",
        "    # state-value V is expectation of return\n",
        "    value = critic(state.detach())\n",
        "    value_prime = critic(state_prime.detach())\n",
        "\n",
        "    ### UPDATE ACTOR ###\n",
        "    optimizer_actor.zero_grad()\n",
        "    \n",
        "    # update actor in direction of higher value (or quality)\n",
        "    quality = reward + gamma*value_prime*done_mask # definition of Q(s,a)\n",
        "\n",
        "    g_prime = quality - value # update expected return G of the action taken \n",
        "    advantage = g_prime\n",
        "    \n",
        "    target = torch.tensor([step_action])\n",
        "    log_prob = torch.log(action)[target]\n",
        "    \n",
        "    # update actor's policy in direction of increasing advantage\n",
        "    actor_loss = -(I * log_prob * advantage.detach()).mean()\n",
        "    actor_loss.backward(retain_graph=True)\n",
        "    optimizer_actor.step()\n",
        "\n",
        "    ### UPDATE CRITIC ###\n",
        "\n",
        "    optimizer_critic.zero_grad()\n",
        "    \n",
        "    # weigh the predicted value of our state against the added/lossed value when we took action a\n",
        "    critic_loss = criterion_critic(value, quality)\n",
        "    critic_loss.backward(retain_graph=True)\n",
        "    optimizer_critic.step()\n",
        "\n",
        "    acc_loss_actor += actor_loss.detach().cpu()\n",
        "    acc_loss_critic += critic_loss.detach().cpu()\n",
        "    action_prime = actor(state_prime)\n",
        "    action = action_prime\n",
        "    state = state_prime\n",
        "    I *= gamma # anneal policy update\n",
        "\n",
        "    if done:\n",
        "      wandb.log({\"Episode Duration\": t+1, \"Actor Loss\": acc_loss_actor/(t+1),\n",
        "                \"Critic Loss\": acc_loss_critic/(t+1)})\n",
        "      break\n",
        "\n",
        "print('Complete')\n",
        "env.close()\n",
        "\n",
        "# Save model to wandb\n",
        "torch.save(actor.state_dict(), os.path.join(wandb.run.dir, 'actor.pt'))\n",
        "torch.save(critic.state_dict(), os.path.join(wandb.run.dir, 'critic.pt'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Complete\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}